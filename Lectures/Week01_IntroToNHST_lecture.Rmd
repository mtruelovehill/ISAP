---
title: "<b> Introduction to Null-Hypothesis <br> Statistical Testing </b>"
subtitle: "Inferential Statistics in Applied Psychology<br> "
author: "Monica Truelove-Hill"
institute: "Department of Clinical Psychology<br>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#4C5376",
  header_color = "#000000",
  header_font_google = google_font("Jost"),
  header_font_weight = 500,
  text_font_google = google_font("Jost", "300", "300i", "500", "500i"),
  code_font_google = google_font("Source Code Pro"),
  text_bold_color = '#4C5376',
  text_slide_number_color = '#db5c68',
  text_font_size = '16pt'
)
```

```{r, echo = F, message = F, warning = F}
library(tidyverse)
library(rstatix)

knitr::opts_chunk$set(dev = 'svg')


baseColor <- '#db5c68'
accent1 <- '#e66c5c'
#accent2 <- '#3B3D7E'
accent2 <- '#4C5376'
accent3 <- '#fdc527'

```

### This Week's Key Topics

+ Making Hypotheses

+ The Null Distribution

+ $\alpha$

+ $p$-Values

---

### Why????

+ Psychology is a science....

  + To answer our research questions, we use **empirical evidence**
  + We take measurements of our construct of interest, then use these observations to draw conclusions


--

  + ...with a lot of uncertainty
  
    + Cannot rely on deterministic models
    + Gather data and look for *consistent patterns* within the data that support (or refute) our ideas
    + However, it's not just the raw values in the data that are important, but the variability in these values
    

---
### Deterministic vs Probabilistic Models

**Example:** Commuting to School

.pull-left.center[

```{r, echo = F, out.width='50%'}
knitr::include_graphics('images/ridingToSchool.jpg')
```
]


.pull-right[
**Deterministic**

$Time = \frac{Distance}{Speed}$


<b>Distance to School:</b> 1.4 miles

<b>Cycling Speed:</b> 15 miles per hour

<b>Time to School: </b> ~ 5.5 minutes

]

---

### Deterministic vs Probabilistic Models

**Example:** Commuting to School

.pull-left.center[

```{r, echo = F, out.width='70%'}
knitr::include_graphics('images/lateToSchool.png')
```
]


.pull-right[

**Probabilistic**

$Time = \frac{Distance}{Speed} + \epsilon$

$\epsilon$: random error

]

---

### Deterministic vs Probabilistic Models

**Example:** Commuting to School

.pull-left.center[

```{r, echo = F, out.width='70%'}
knitr::include_graphics('images/lateToSchool.png')
```
]


.pull-right[

**Probabilistic**

$Time = \frac{Distance}{Speed} + \epsilon$

$\epsilon$: random error

```{r, echo = F, fig.height=4, fig.width = 6}
set.seed(4)
x <- data.frame(x = rnorm(30, mean = 9, sd = 2))
ggplot(dat = x, aes(x)) + geom_histogram(bins = 10, color = accent1 , fill = baseColor) +
  labs(x = 'Time to School (minutes)', y = 'Frequency') + 
  scale_x_continuous(breaks=seq(0, 16, by=2)) + 
  coord_cartesian(xlim = c(0, 16)) +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14, face = 'bold'),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  geom_vline(xintercept = 5.5, color = accent2, linewidth = 1, lty = 'dashed')
```


]


---
### Formulating a Model

+ **Model:** a formal representation of the way the world works

  + Your expectations around your construct of interest

--

+ When you make an alternative hypothesis, you're proposing a model

  + **Alternative Hypothesis ( $H_1$ ):** Your proposed model about how things work

--

+ An alternative hypothesis is always partnered with a null hypothesis

  + **Null Hypothesis ( $H_0$ ):** What you would expect if your model is false

--

+ What does this have to do with statistics?

---
### Testing the Model

+ Statistics provide a way of testing your proposed model.

+ Null-hypothesis statistical tests of significance assess strength of the evidence against the null hypothesis

  + The burden of proof is on the alternative hypothesis

  + Presume null hypothesis is accurate until there is sufficient evidence that it is not

---
### Assessing Evidence

+ Statistical tests assess evidence against the null hypothesis using **probability**

  + How likely is it that the null hypothesis is true, given the observed data?
  
--

+ In psychology, we gather data and calculate the likelihood of these data occurring under the constraints of the null hypothesis

+ We then make a decision of whether to reject the null hypothesis based around this likelihood.
  
  + If the observed data is unlikely to occur under the null hypothesis, we consider this sufficient evidence to reject it
  
  + If the observed data is likely to occur under the null hypothesis, we do not reject it, because we haven't gathered sufficient evidence against it.

---
class: center, inverse, middle

## Questions?

---
### Probability Distributions

+ Probability distributions allow you to make a general estimate about the likelihood of an observation occurring.

.pull-left[


```{r, echo=F, fig.width=6.5, fig.height=4, warning = F}
set.seed(208)
dat <- data.frame(x=sample(c('Heads', 'Tails'), size = 40, replace = T, prob = c(.5, .5)), y = rnorm(n = 40))
pDat <- data.frame(Outcome = unique(dat$x), Probability = as.numeric(table(dat$x)/nrow(dat)))

ggplot(pDat, aes(x=Outcome, y=Probability)) + 
  geom_bar(stat='identity', fill=accent1) +
  scale_y_continuous(breaks=seq(0, .7, .1)) +
  theme(axis.text=element_text(size = 14),
        axis.title.y=element_text(size = 16, face = 'bold'),
        axis.title.x = element_blank())
```



**Categorical Distribution:** Shows the probability of randomly selecting a specific observation from a sample
]

.pull-right[

```{r, echo=F, fig.width=6.5, fig.height=4, warning = F}
ggplot(dat, aes(x=y)) +
  stat_function(fun = dnorm, geom = 'line', args = c(mean=mean(dat$y), sd = sd(dat$y)), linewidth = 1.5, color = accent3) +
  scale_x_continuous(limits = c(-3, 3)) +
  scale_y_continuous(breaks=seq(0, .5, by = .1)) +
  theme(axis.text.x = element_text(size=14),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title = element_blank())
```

**Continuous Distribution:** Shows the probability of randomly selecting a range of values from a sample
]

---
### Continuous Probability Distributions

.center[

```{r, echo = F, fig.height=4.5, fig.width=8}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun=dnorm, geom = "line", linewidth = 1.5, color = accent3) +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14, face = 'bold'),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank()) +
  geom_hline(yintercept = 0, linewidth = 1.5)

```
]

> Why a range? Why can't we compute the probability of a single score?

---
### Continuous Probability Distributions

.center[
```{r, echo = F, fig.height = 4.5, fig.width = 8}

ggplot(data.frame(x = c(-4, 4)), aes(x = x)) + stat_function(fun=dnorm, geom = "line", linewidth = 1.5, color = accent3) +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14, face = 'bold'),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank()) +
  geom_hline(yintercept = 0, linewidth = 1.5) +
  geom_vline(xintercept = 2.4822, color = baseColor, linewidth = 1.5)

```
]

> Why a range? Why can't we compute the probability of a single score?

---
### Continuous Probability Distributions

.center[
```{r, echo = F, fig.height = 4.5, fig.width = 8}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) + stat_function(fun=dnorm, geom = "line", linewidth = 1.5, color = accent3) +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14, face = 'bold'),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank()) +
  geom_hline(yintercept = 0, linewidth = 1.5) +
  geom_vline(xintercept = 2.4822, color = baseColor, linewidth = 1.5) + coord_cartesian(xlim=c(2, 3))
```
]

> Why a range? Why can't we compute the probability of a single score?

---
### Continuous Probability Distributions

.center[
```{r, echo = F, fig.height = 4.5, fig.width = 8}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) + stat_function(fun=dnorm, geom = "line", linewidth = 1.5, color = accent3) +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14, face = 'bold'),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank()) +
  geom_hline(yintercept = 0, linewidth = 1.5) +
  geom_vline(xintercept = 2.4822, color = baseColor, linewidth = 1.5) + coord_cartesian(xlim=c(2.45, 2.52))
```
]

> Why a range? Why can't we compute the probability of a single score?

---
### Continuous Probability Distributions

.center[
```{r, echo = F, fig.height = 4.5, fig.width = 8}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) + stat_function(fun=dnorm, geom = "line", linewidth = 1.5, color = accent3) +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14, face = 'bold'),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank()) +
  geom_hline(yintercept = 0, linewidth = 1.5) +
  geom_vline(xintercept = 2.4822, color = baseColor, linewidth = 1.5) + coord_cartesian(xlim=c(2.475, 2.488))
```
]

> Why a range? Why can't we compute the probability of a single score?

---

### Continuous Probability Distributions

.center[
```{r, echo = F, fig.height = 4.5, fig.width = 8}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) + stat_function(fun=dnorm, geom = "line", linewidth = 1.5, color = accent3) +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14, face = 'bold'),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank()) +
  geom_hline(yintercept = 0, linewidth = 1.5) +
  geom_vline(xintercept = 2.4822, color = baseColor, linewidth = 1.5) + coord_cartesian(xlim=c(2.475, 2.488))
```
]

> Why a range? Why can't we compute the probability of a single score?

> The probability of an observation is split between all possible values

> Technically, continuous values go on infinitely, so the probability of any single value is approximately 0

---

### Continuous Probability Distributions

.pull-left[
+ We can instead calculate the probability of a range by computing the **area under the curve (AUC)** within that range.

+ Imagine we have a range of observations whose values fall between 8 and 32 that are distributed as pictured.

]

.pull-right[
```{r, echo = F, fig.height=4}
testDat <- ggplot(data.frame(x = c(8, 32)), aes(x = x)) +
  stat_function(fun=dnorm, args= list(20, 3), geom = "line", linewidth = 1.5, color = accent3) +
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 14, face = 'bold'),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank()) +
  geom_hline(yintercept = 0, linewidth = 1.5)

testDat
```
]

--

> **Test Your Understanding**: If you randomly selected an observation from this distribution, what numbers might you expect to see? What numbers would surprise you if you randomly picked them? 

---
### Continuous Probability Distributions

.pull-left[

+ We can instead calculate the probability of a range by computing the **area under the curve (AUC)** within that range.

+ Imagine we have a range of observations whose values fall between 8 and 32 that are distributed as pictured.

+ If we randomly select an observation from our data, what is the likelihood it will be at least 25?


]

.pull-right[

```{r, echo = F, fig.height=4}
testDat +
    geom_vline(xintercept = 25, color = accent2, linewidth = 1.5)
```

]

--

> **Test Your Understanding**: What is the range of values we have selected to calculate the probability of?

---

### Continuous Probability Distributions

.pull-left[
+ We can instead calculate the probability of a range by computing the **area under the curve (AUC)** within that range.

+ Imagine we have a range of observations whose values fall between 8 and 32 that are distributed as pictured.

+ If we randomly select an observation from our data, what is the likelihood it will be at least 25?

+ ~95% of our observations are less than 25. This means we only have about a 5% chance of randomly selecting an observation of at least 25. 

]

.pull-right[

```{r, echo = F, fig.height=4}
testDat +
  geom_area(stat = 'function', fun = dnorm, args= list(20, 3), fill = accent3, xlim=c(8, 25), alpha = .8) +
  geom_area(stat = 'function', fun = dnorm, args= list(20, 3), fill = accent1, xlim=c(25, 30), alpha = .8) +
  annotate('text', label = '95%', x = 20, y = .05, color = 'white', size = 7) +
  annotate('text', label = '5%', x = 26, y = .01, color = 'white', size = 7) +
  geom_vline(xintercept = 25, color = accent2, linewidth = 1.5)
```
]

---
### Continuous Probability Distributions

.pull-left[

```{r, echo = F, fig.height=5}
testDat + geom_vline(xintercept = 24, color = accent2, linewidth = 1.5)
```

]

.pull-right[
```{r, echo = F, fig.height=5}
testDat + geom_vline(xintercept = 18, color = accent2, linewidth = 1.5)
```

]

> **Test Your Understanding:** In this distribution (pictured twice), is it more unusual to have a score of 24 or more or a score of 18 or less?

---

### Continuous Probability Distributions

.pull-left[
```{r, echo = F, fig.height=5}
testDat +
  geom_area(stat = 'function', fun = dnorm, args= list(20, 3), fill = accent3, xlim=c(8, 24), alpha = .8) +
  geom_area(stat = 'function', fun = dnorm, args= list(20, 3), fill = accent1, xlim=c(24, 30), alpha = .8) +
  annotate('text', label = '91%', x = 18, y = .02, color = 'white', size = 7) +
  annotate('text', label = '9%', x = 25, y = .02, color = 'white', size = 7)+
  geom_vline(xintercept = 24, color = accent2, linewidth = 1.5)
```
]

.pull-right[
```{r, echo = F, fig.height=5}
testDat +
  geom_area(stat = 'function', fun = dnorm, args= list(20, 3), fill = accent1, xlim=c(8, 18), alpha = .8) +
  geom_area(stat = 'function', fun = dnorm, args= list(20, 3), fill = accent3, xlim=c(18, 30), alpha = .8) +
  annotate('text', label = '25%', x = 16, y = .02, color = 'white', size = 7) +
  annotate('text', label = '75%', x = 22, y = .02, color = 'white', size = 7) +
  geom_vline(xintercept = 18, color = accent2, linewidth = 1.5)
```
]

> **Test Your Understanding:** Is it more unusual to have a score of 24 or more or a score of 18 or less?

---
### Continuous Probability Distributions

+ Why does this matter?

--

+ Remember, the statistical tests we will learn about work by testing the probability of our results given the null hypothesis

  + In other words, if the null hypothesis is true, how likely is it that we would get results at least as unusual as ours?

+ If our results are unusual enough, we reject the null hypothesis.

---
class: center, inverse, middle

## Questions?

---
### Testing Against the Null Hypothesis

+ In order to make a decision whether our data/values are unusual, we need to know what values we would expect if the null hypothesis were true.


--

> **Test Your Understanding:** Given the following distributions, would you consider a value of 20 to be unusual? 


--
.pull-left[
```{r, echo = F, fig.height=4}
set.seed(211)
ggplot(data.frame(x=rnorm(100, mean = 18, sd = 3)), aes(x)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = .75, fill = accent3, color = accent2) +
  geom_density(color = accent2, linewidth = 1) + 
  geom_vline(xintercept = 20, color = baseColor, linewidth = 1) +
  theme(axis.text = element_text(size = 14),
        axis.title = element_text(size = 16, face = 'bold'),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```
]

--

.pull-right[
```{r, echo = F, fig.height=4}
set.seed(311)
ggplot(data.frame(x=rnorm(100, mean = 13, sd = 3)), aes(x)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = .75, fill = accent3, color = accent2) +
  geom_density(color = accent2, linewidth = 1) + 
  geom_vline(xintercept = 20, color = baseColor, linewidth = 1) +
  theme(axis.text = element_text(size = 14),
        axis.title = element_text(size = 16, face = 'bold'),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```
]



???

+ A value of 20 tells us nothing without context


---
### Null Distributions

+ The distribution against which the value we've obtained from our data is evaluated is known as the **null distribution.**

+ Each statistical test is associated with it's own null distribution

+ The null distribution is the probability distribution of a statistic **given that the null hypothesis is true**. 

---

### Examples of Null Distributions

.pull-left.center[
```{r, echo = F, fig.height=3, fig.width=5}
ggplot(data.frame(x=c(0, 10)), aes(x=x)) +
  stat_function(fun=dchisq, geom='line', args=c(df = 2), color = accent2, linewidth=1.5) +
  scale_y_continuous(limits=c(0, .52), breaks = seq(0, .5, .1)) +
  labs(x = expression(chi^2)) +
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.x = element_text(size = 16, face = 'bold'),
        axis.text.x = element_text(size = 14))

```
]


.pull-right.center[
```{r, echo = F, fig.height=3, fig.width=5}
ggplot(data.frame(x=c(-6, 6)), aes(x=x)) +
  stat_function(fun=dt, geom='line', args=c(df = 10), color = accent3, linewidth=1.5) +
  labs(x = 't-statistic') +
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.x = element_text(size = 16, face = 'bold'),
        axis.text.x = element_text(size = 14))

```
]

.center[
```{r, echo = F, fig.height=3, fig.width=5}
ggplot(data.frame(x=c(0, 5)), aes(x=x)) +
  stat_function(fun=df, geom='line', args=c(df1 = 5, df2=10), color = baseColor, linewidth=1.5) +
  labs(x = 'F-statistic') +
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.x = element_text(size = 16, face = 'bold'),
        axis.text.x = element_text(size = 14))

```
]

---
### Comparing Results to Null Distribution

When you run a statistical test, you:

  1. Compute a test statistic based on your data
  
  2. Use the null distribution to calculate the probability of obtaining this statistic if the null were true
  
  3. Use this probability to make a decision whether to reject the null hypothesis.
  
---
### Comparing Results to Null Distribution

.pull-left[.center[
```{r, echo = F, fig.height=4, fig.width=6}
baset <- ggplot(data.frame(x=c(-6, 6)), aes(x=x)) +
  stat_function(fun=dt, geom='line', args=c(df = 50), linewidth=1.5, color = accent3) +
  labs(x = 'statistic') +
  theme(axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.x = element_text(size = 12, face = 'bold'),
        axis.text.x = element_text(size = 10)) +
  geom_hline(yintercept = 0, linewidth = 1.5)

baset + geom_vline(xintercept = .75, linewidth = 1.5, color = baseColor) +
  annotate('text', label = 'statistic = 0.75', x = 2.25, y = .3, size = 5)
```
]]


---
### Comparing Results to Null Distribution

.pull-left[.center[
```{r, echo = F, fig.height=4, fig.width=6}
(ns <- baset +
  annotate('text', label = 'statistic = 0.75', x = 2.25, y = .3, size = 5) +
  geom_area(stat = 'function', fun = dt, args= list(df = 50), xlim=c(-6, .75), fill = accent3, alpha = .8) +
  annotate('text', label = '~77%', x = -.4, y = .05, color = 'white', size = 5) +
  geom_area(stat = 'function', fun = dt, args= list(df = 50), xlim=c(.75, 6), fill = accent1, alpha = .8) +
  annotate('text', label = '~23%', x = 1.4, y = .05, color = 'white', size = 5) +
  geom_vline(xintercept = .75, linewidth = 1.5, color = baseColor))
```
]]

---

### Comparing Results to Null Distribution

.pull-left[.center[
```{r, echo = F, fig.height=4, fig.width=6}
ns
```
<B>Not so unusual.</B>
]]

--

.pull-right[.center[
```{r, echo = F, fig.height=4, fig.width=6}
(sigPlot <- baset +
  geom_vline(xintercept = 3.05, linewidth = 1.5) +
  annotate('text', label = 't = 3.05', x = 4.5, y = .3, size = 5))
```
]]

---

### Comparing Results to Null Distribution

.pull-left[.center[
```{r, echo = F, fig.height=4, fig.width=6}
ns
```
<B> Not so unusual. </B>
]]

.pull-right[.center[
```{r, echo = F, fig.height=4, fig.width=6}
baset +
  geom_vline(xintercept = 3.05, linewidth = 1.5, color = accent2) +
  annotate('text', label = 'statistic = 3.05', x = 4.5, y = .3, size = 5) +
  geom_area(stat = 'function', fun = dt, args= list(df = 50), xlim=c(-6, 3.05), fill = accent3, alpha = .8) +
  annotate('text', label = '~99.8%', x = 0, y = .05, color = 'white', size = 5)

# sigPlot +
#   geom_area(stat = 'function', fun = dt, args= list(df = 50), xlim=c(-6, 3.05), fill = accent3, alpha = .8) +
#   annotate('text', label = '~99.8%', x = 0, y = .05, color = 'white', size = 5)
```
<B> Definitely unusual! </B>
]]

---

class: center, inverse, middle

## Questions?

---
### $p$ - values

.pull-left[
+ The $p$-value is the **probability** of obtaining data at least as extreme as ours, given that the null hypothesis is true.

+ It is not:
  + The likelihood that the null hypothesis is true.
  + The likelihood that our hypothesis is false
]

---

### $p$ - values

.pull-left[
+ The $p$-value is the **probability** of obtaining data at least as extreme as ours, given that the null hypothesis is true.

+ It is not:
  + The likelihood that the null hypothesis is true.
  + The likelihood that our hypothesis is false

+ It provides insight into the strength of our evidence against the null hypothesis. 

+ If the $p$-value is below a certain threshold, we consider the results to be **significant**, and the null hypothesis is rejected

]

--

.pull-right[
```{r, echo=F, fig.height=4, fig.width=6}
ns + annotate('text', label = 'p = .230', x = 4.5, y = .05, size = 6, color = accent2) +
  annotate('segment', x = 2.25, xend = 3.2, y = .045, yend = .045, color = accent2,
           arrow = arrow(type = "closed", length = unit(0.02, "npc"), ends = 'first'))
```
]

---
## $\alpha$

+ The 'threshold' for significance is known as $\alpha$

+ If the $p$-value is less than the predetermined $\alpha$ threshold, the null hypothesis is rejected

+ But what, exactly, does the $\alpha$ value reflect?


---
### Errors

+ Whether the decision is made to either to reject or not reject the null hypothesis, you may be making an error.

+ Basically, what you find in your sample isn't actually what is true of the population

--

+ Two types of errors:

  + **Type I Error:** You reject a true null hypothesis (i.e., you suggest there is an effect or association when there really isn't)
  
  + **Type II Error:** You don't reject a false null hypothesis (i.e., you don't find an effect or association that actually exists)


---

## $\alpha$

+ $\alpha$ reflects the probability of making a Type I error

+ It is the likelihood of rejecting the null when it is actually true 

+ You choose your desired $\alpha$ value before you run any analyses.
  
  + The most commonly used $\alpha$ is .05, but may sometimes be set at .01 or .001.
  
+ If $\alpha$ is set at .05, it means there is a 5% risk of rejecting the null hypothesis when it shouldn't be rejected. 

---
class: center, middle, inverse

## Questions?
---

### One-Tailed vs Two-Tailed Tests

.pull-left[

+ When evaluating evidence against $H_0$, our threshold for what values are considered to be *extreme* depends on whether we're running a one-tailed or two-tailed test.

]

---
### One-Tailed vs Two-Tailed Tests

.pull-left[

+ When evaluating evidence against $H_0$, our threshold for what values are considered to be *extreme* depends on whether we're running a one-tailed or two-tailed test.

+ With a **one-tailed test**, we're only interested in values in a single tail of a distribution

  + If $\alpha$ is set at .05, we check that our observation is in the highest 5% of values *or* the lowest 5% of values

]

.pull-right[

```{r, echo = F, fig.height=2.5}
ggplot(data = data.frame(t = c(-5, 5)), aes(x=t)) +
  stat_function(fun=dt, geom='line', args=c(df = 29), color = accent3, linewidth=1.5) +
  geom_area(stat = 'function', fun = dt, args= list(df = 29), fill = accent3,
            xlim=c(qt(.05, df = 29, lower.tail = F), 5), alpha = .8) +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        axis.line.y = element_blank(),
        axis.ticks = element_blank()) +
  annotate('text', x = 3, y = .3, parse = T, label = paste('alpha == .05'), size = 8, color = accent2) +
  geom_hline(yintercept = 0, linewidth = 1.5)
```

.center[
<b>OR</b>
]

```{r, echo = F, fig.height=2.5}
ggplot(data = data.frame(t = c(-5, 5)), aes(x=t)) +
  stat_function(fun=dt, geom='line', args=c(df = 29), color = accent3, linewidth=1.5) +
  geom_area(stat = 'function', fun = dt, args= list(df = 29), fill = accent3,
            xlim=c(-5, qt(.05, df = 29)), alpha = .8) +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        axis.line.y = element_blank(),
        axis.ticks = element_blank()) +
  annotate('text', x = 3, y = .3, parse = T, label = paste('alpha == .05'), size = 8, color = accent2) +
  geom_hline(yintercept = 0, linewidth = 1.5)
```
]

---
### One-Tailed vs Two-Tailed Tests

.pull-left[

+ When evaluating evidence against $H_0$, our threshold for what values are considered to be *extreme* depends on whether we're running a one-tailed or two-tailed test.

+ With a **two-tailed test**, we're interested in values in both tails of a distribution

  + If $\alpha$ is set at .05, we check that our observation is in the *most extreme* 5% of values, whether they are higher or lower
  
  + 5% is split equally between the two tails

]

.pull-right[

```{r, echo = F, fig.height=4}
ggplot(data = data.frame(t = c(-5, 5)), aes(x=t)) +
  stat_function(fun=dt, geom='line', args=c(df = 29), color = accent3, linewidth=1.5) +
  geom_area(stat = 'function', fun = dt, args= list(df = 29), fill = accent3,
            xlim=c(qt(.025, df = 29, lower.tail = F), 5), alpha = .8) +
  geom_area(stat = 'function', fun = dt, args= list(df = 29), fill = accent3,
            xlim=c(-5, qt(.025, df = 29)), alpha = .8) +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        axis.line.y = element_blank(),
        axis.ticks = element_blank()) +
  annotate('text', x = 3, y = .3, parse = T, label = paste('alpha == .05'), size = 8, color = accent2) +
  annotate('text', x = 3.25, y = .05, parse = T, label = paste('alpha/2 == .025'), size = 6, color = accent2) +
  annotate('text', x = -3.25, y = .05, parse = T, label = paste('alpha/2 == .025'), size = 6, color = accent2) +
  annotate('segment', x= -2.25, xend = 3, y = .06, yend = .28, color = accent2, linewidth = 1, linetype = 'dashed') +
  annotate('segment', x= 2.4, xend = 3, y = .06, yend = .28, color = accent2, linewidth = 1, linetype = 'dashed') +
  geom_hline(yintercept = 0, linewidth = 1.5)
```

]

---
### Putting it Together

+ When we run a statistical test, we compare our results against the null distribution, which is the probability of the results given the null hypothesis is true.

+ If the probability of our outcome (the $p$-value) is less than the $\alpha$ threshold, it means the outcome falls into the range that we've designated as extreme.

+ Because it's really unlikely that these results would occur if the null hypothesis were true, we consider this to be sufficient evidence against the null hypothesis, and we reject it.

---
class: center, inverse, middle

## Questions?